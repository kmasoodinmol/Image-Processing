{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar_data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RRU10mw4XaVT"
      },
      "source": [
        "#**CNN Image processing with NOISY data...\n",
        "Dr. Munir Ahmad, Lahore, PKISTAN\n",
        "munirahm@gmail.com**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OvozTliU64e"
      },
      "source": [
        "# **We can import the needed libraries first....**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Mh80UCyxM_O"
      },
      "source": [
        "from keras.datasets import cifar10\n",
        "from skimage.util import random_noise\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from keras.utils import np_utils\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, BatchNormalization, Activation\n",
        "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
        "from keras.constraints import maxnorm\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xkdkaDiJVLoP"
      },
      "source": [
        "# **Load CIFAR10 dataset and normlize traning and test data....**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKiLp9HJxThA"
      },
      "source": [
        "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
        "X_train = X_train.astype('float32')\n",
        "X_test = X_test.astype('float32')\n",
        "X_train = X_train / 255.0\n",
        "X_test = X_test / 255.0\n",
        "y_train = np_utils.to_categorical(y_train)\n",
        "y_test = np_utils.to_categorical(y_test)\n",
        "nX_train = X_train\n",
        "nX_test = X_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_52bGi1lVXLC"
      },
      "source": [
        "# **Add noise to the dataset with variance of 0.01**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ojMT7UC5gxry"
      },
      "source": [
        "for img in range(X_train.shape[0]):\n",
        "  nX_train[img, :, :, :] = random_noise(X_train[img, :, :, :], mode='gaussian', seed=None, clip=True, var=0.01)\n",
        "for img in range(X_test.shape[0]):\n",
        "  nX_test[img, :, :, :] = random_noise(X_test[img, :, :, :], mode='gaussian', seed=None, clip=True, var=0.01)\n",
        "  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcRQq9bkVhXD"
      },
      "source": [
        "# **Devise a model based on Vgg...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cTdhCOYTywKq"
      },
      "source": [
        "drp = 0.2\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Conv2D(32, (3,3), input_shape = X_train.shape[1:], padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(64, (3,3), padding='same'))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(MaxPooling2D(2,2))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Conv2D(128, (3,3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Flatten())\n",
        "\n",
        "model.add(Dense(256, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(128, kernel_constraint=maxnorm(3)))\n",
        "model.add(Activation('relu'))\n",
        "model.add(Dropout(drp))\n",
        "model.add(BatchNormalization())\n",
        "\n",
        "model.add(Dense(10, 'softmax'))\n",
        "\n",
        "epochs = 25\n",
        "optimizer = 'adam'\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tcxXFKSoTiBV"
      },
      "source": [
        "print(model.summary())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fx18k5xYYnsf"
      },
      "source": [
        "#**Modelrun with nioseless data...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eDwaoPwXYGy-"
      },
      "source": [
        "np.random.seed(21)\n",
        "history1 = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UbyzKTQoYwBA"
      },
      "source": [
        "#**Noiseless results...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dSKUYqn6YRa7"
      },
      "source": [
        "# list all data in history\n",
        "print(history1.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history1.history['accuracy'])\n",
        "plt.plot(history1.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history1.history['loss'])\n",
        "plt.plot(history1.history['val_loss'])\n",
        "plt.title('model sloss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHdnIQthT8bZ"
      },
      "source": [
        "np.random.seed(21)\n",
        "history = model.fit(nX_train, y_train, validation_data=(nX_test, y_test), epochs=epochs, batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3DsXNgEVu5O"
      },
      "source": [
        "# **Display accuracy and loss curves...**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8CvvSeDKDk6"
      },
      "source": [
        "# list all data in history\n",
        "print(history.history.keys())\n",
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}